//simulacro6

// 1. Leer la tabla fbi.resultado_avro existente en hive, con spark
// Realizar un filtro para quedarnos solo con los años que superen los 140k delitos. Guardar en parquet + snappy

//para poder leer las tablas de hive, primero hacemos un softlink de hive-site a la conf de spark
// Luego definimos el nuevo sqlContext que permita leer en hive

sudo ln -s /usr/lib/hive/hive-site.xml /usr/lib/spark/conf/hive-site.xml

val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)

val lectura = sqlContext.table("fbi.resultado_avro")

val solution = lectura.where($"delitos">140000)

sqlContext.setConf("spark.sql.parquet.compression.codec", "snappy")

solution.write.parquet("/user/cert/simulacro6/ej1")

// 2. Importar de la tabla customers, los campos customer_id, customer_fname, customer_lname, customer_city, customer_state, customer_zipcode.
// Traer solo los registros cuyo customer_id sea superior a 5000. Un solo mapper. Formato avro + snappy

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table customers \
--target-dir /user/cert/simulacro6/ej2 \
--columns customer_id,customer_fname,customer_lname,customer_city,customer_state,customer_zipcode \
--where 'customer_id > 5000' \
--as-avrodatafile \
--compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
-m 1


// 3. Con los datos importados en 2), encontrar aquellos clientes cuyo nombre y apellido comiencen con la misma letra. Output: customer_fname, customer_lname, customer_location( customer city + "," + customer_state + "," + cuztomer_zipcode). Guardar en tsv

import com.databricks.spark.avro._

val lectura = sqlContext.read.avro("/user/cert/simulacro6/ej2")

lectura.registerTempTable("tomate")
//vamos a hacer una primera aproximacion para sacar los registros que tengan misma inicial de nombre y apellido
val filtroletras = sqlContext.sql("select * from tomate where substring(customer_fname,0,1)==substring(customer_lname,0,1)")
//haciendo un show de esto, vemos que vamos por buen camino
//vamos ahora a unir estos campos para la localizacion
val solucion = sqlContext.sql("select customer_fname, customer_lname, (customer_city + ' ' + customer_state + ' ' + customer_zipcode) as location from tomate where substring(customer_fname,0,1)==substring(customer_lname,0,1)")
//nos pide guardarlo en tsv, primero lo pasaremos a rdd y lo mapearemos separando por tabs
val myRDD = solucion.rdd.map(x => x(0) + "\t" + x(1) + "\t" + x(2))

myRDD.coalesce(1).saveAsTextFile("/user/cert/simulacro6/ej3")

// 4. Leer purchase.json. 
//Realizar las acciones que nos devuelvan todos los productos cuyo itemType sea Games, y obtener el dinero producido por cada producto. Guardar en json + gzip

val lectura = sqlContext.read.json("/user/cert/problem4/data/purchase.json")
//tenemos que filtrar por itemType = Games
val filtro = lectura.where($"itemType"==="Games")
//queremos juntar todos los productos con el mismo nombre, y ver cuantos se han vendido. Luego hallaremos el dinero recaudado, es necesario hacer algo con el precio en la funcion de agg para que aparezca como columna de la tabla
val agrupado = filtro.groupBy("productName").agg(sum("quantity").as("total_productos"), round(avg("price")).as("precio"))
//vamos a calcular ahora el dinero recaudado
//pasamos el DF a RDD para poder calcular el precio total con un mapeo
val base = agrupado.map(x => (x(0), x(1).toString.toInt*x(2).toString.toFloat))
//ahora vamos a darle estructura otra vez para guardarlo en json
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

val str = StructType(Array(
  StructField("producto", StringType),
  StructField("recaudacion", FloatType)
))

val reg = base.map(x => Row(x._1, x._2))

val myDF = sqlContext.createDataFrame(reg, str)
//finalmente guardamos en json
myDF.coalesce(1).toJSON.saveAsTextFile("/user/cert/simulacro6/ej4", classOf[org.apache.hadoop.io.compress.GzipCodec])


// 5. Con el fichero OlympicAthletes.csv calcular el numero total de medallas que consiguió cada pais cada año, ordenar de forma descencente. Guardar en csv

val oly = sc.textFile("/user/cloudear/data/olympic/OlympicAthletes.csv").
			map(_.split(','))
val pair = oly.map(x => ((x(2),x(3).toInt),x(9).toInt)).
			reduceByKey((x,y) => x+y).map (x => (x._2,x._1)).
			sortByKey(ascending=false).map(x => (x._2,x._1))
val solucion = pair.map(x=> x._1._1 + "," + x._1._2 + "," + x._2) 
solucion.coalesce(1).saveAsTextFile("/user/cert/simulacro6/ej5")


// 6. Exportar los resultados del ejercicio 5 a una tabla mysql

mysql -u cloudera -pcloudera

use ejercicios;

create table medals(
country varchar(45),
year int,
medals int)
;

sqoop export \
--connect jdbc:mysql://localhost:3306/ejercicios \
--username cloudera --password cloudera \
--table medals \
--export-dir /user/cert/simulacro6/ej5 \
--fields-terminated-by ','


// 7. Encontrar todos los customer que no han puesto ningun order. Output: customer_fname + " " + customer_lname. Guardar en parquet + gzip
import com.databricks.spark.avro._

val cust = sqlContext.read.avro("/user/cert/simulacro3/data/customers")
val ord = sqlContext.read.avro("/user/cert/simulacro3/data/orders")

val joined = cust.join(ord, cust("customer_id")===ord("order_customer_id"), "left_outer")

val customersNoOrders = joined.where($"order_id".isNull).
						select("customer_fname", "customer_lname")

customersNoOrders.registerTempTable("lalala")

val sol = sqlContext.sql("select concat_ws(' ', customer_fname, customer_lname) as nombre from lalala")

sqlContext.setConf("spark.sql.parquet.compression.codec", "gzip")
sol.coalesce(1).write.parquet("/user/cert/simulacro6/ej7")
