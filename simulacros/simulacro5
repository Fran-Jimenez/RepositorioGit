//simulacro 5

// 1. IMPORTAR LA TABLA PRODUCTS, EN FORMATO AVRO + SNAPPY. TRAER SOLO AQUELLAS REGISTROS CUYO PRECIO SEA INFERIOR A 150. TRAER LAS COLUMNAS, PRODUCT_ID, PRODUCT_CATEGORY_ID, PRODUCT_NAME, PRODUCT_PRICE. 1 SOLO MAPPER

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table products \
--target-dir /user/cert/simulacro5/ej1 \
--columns product_id,product_category_id,product_name,product_price \
--where 'product_price < 150' \
--as-avrodatafile \
--compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
-m 1



// 2. CON LA TABLA DEL EJERCICIO 1. QUEDARNOS SOLO CON PRODUCT_NAME Y PRODUCT_PRICE, Y REALIZAR UNA EXPORTACION A MYSQL.

import com.databricks.spark.avro._

val prod = sqlContext.read.avro("/user/cert/simulacro5/ej1")
val col = prod.rdd.map(x => x(2) + "," + x(3).toString)
col.saveAsTextFile("/user/cert/simulacro5/ej2")


mysql -u cloudera -pcloudera

use ejercicios;

create table prod_mysql(
  product_name varchar(255),
  product_price float(45));

exit; 

sqoop export \
--connect jdbc:mysql://localhost:3306/ejercicios \
--username cloudera --password cloudera \
--table prod_mysql \
--export-dir /user/cert/simulacro5/ej2 \
--fields-terminated-by ','


// 3. CREAR UNA TABLA DE HIVE QUE RECOJA LOS DATOS DEL EJERCICIO 1 (PRODUCTS_HIVE). TRAS ELLO HACER UNA QUERY QUE NOS DEVUELVA AQUELLOS PRODUCTOS CON PRODUCT_CATEGORY_ID SEA PAR. GUARDAR EL RESULTADO DE LA QUERY EN HDFS COMO FICHERO DE CSV

hive

create external table products_hive (
  product_id  int,
  product_category_id int,
  product_name string,
  product_price float)
 stored as avro
 location '/user/cert/simulacro5/ej1'


insert overwrite directory '/user/cert/simulacro5/ej3' row format delimited fields terminated by ',' stored as textfile select * from products_hive where product_category_id in (2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58);


OTRA FORMA 
insert overwrite directory '/user/cert/simulacro5/ej3' row format delimited fields terminated by ',' stored as textfile select * from products_hive where product_category_id % 2 = 0


// 4. CON LA TABLA ORDERS, REALIZAR UN FILTRO QUE NOS DEVUELVA TODOS LOS REGISTROS CON ORDER_DATE ENTRE 2012 Y 2014. GUARDAR EN PARQUET + GZIP

import com.databricks.spark.avro._

val ord = sqlContext.read.avro("/user/cert/simulacro3/data/orders")

val formateo = ord.select($"order_id", to_date(from_unixtime($"order_date"/1000)), $"order_customer_id", $"order_status")

val nuevo = formateo.withColumnRenamed("to_date(from_unixtime((order_date / 1000),yyyy-MM-dd HH:mm:ss))", "order_date_formmated")

val solucion = nuevo.where($"order_date_formmated" > "2011-12-31").where($"order_date_formmated" < "2015-01-01")

sqlContext.setConf("spark.sql.parquet.compression.codec", "gzip")
solucion.write.parquet("/user/cert/simulacro5/ej4")



// 5. LEER EL ARCHIVO PURCHASE.JSON DE HDFS Y REALIZAR UN FILTRO PARA CONSEGUIR SOLO LOS REGISTROS CUYO PRECIO SEA MENOR DE 100. EL OUTPUT DEBE TENER SOLO LAS COLUMNAS ID, CUSTID, PORDUCTNAME, PRICE Y QUANTITY.
GUARDAR EN AVRO + SNAPPY

val jsondata = sqlContext.read.json("/user/cert/problem4/data/purchase.json").where($"price" < 100).select("ID", "CustID", "productName", "quantity")

import com.databricks.spark.avro._

sqlContext.setConf("spark.sql.avro.compression.codec", "snappy")

jsondata.write.avro("/user/cert/simulacro5/ej5")



// 6.  CON EL FICHERO OLYMPICATHLETES, ENCONTRAR EL PODIO DE LOS ATLETAS CON MEJORES MEDALLAS (0R0, PLATA, BRONCE), DE TAL FORMA QUE EL PRIMERO SEA QUIEN TIENE MAS MEDALLAS DE ORO, LUEGO LE SIGA EL SIGUIENTE CON MAS OROS, Y ASI SUCESIVAMENTE, EN CASO DE EMPATE EN OROS APARECERA PRIMERO EL QUE TENGA MAS PLATAS.....
GUARDAR EN CSV + GZIP

val oly = sc.textFile("/user/cloudear/data/olympic/OlympicAthletes.csv").map(_.split(',')).map(x => (x(0),(x(6).toInt,x(7).toInt,x(8).toInt)))

val solution = oly.reduceByKey( (x,y) =>(x._1+y._1,x._2+y._2,x._3+y._3) ).map(x => (x._1, x._2._1,x._2._2,x._2._3))

import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

val str = StructType(Array(
  StructField("names", StringType),
  StructField("golds", IntegerType),
  StructField("silvers", IntegerType),
  StructField("bronzes", IntegerType)
  ))

val reg = solution.map(x => Row(x._1, x._2, x._3, x._4))

val myDF = sqlContext.createDataFrame(reg,str)

val mySolution = myDF.orderBy($"golds".desc, $"silvers".desc, $"bronzes".desc).limit(3)

mySolution.rdd.map(x=> x(0) + "," + x(1) + "," + x(2) + "," + x(3)).saveAsTextFile("/user/cert/simulacro5/ej6" classOf[org.apache.hadoop.io.compress.GzipCodec])

// 7. DEL FICHERO CRIMES, GUARDAR LAS 5 PRIMERAS COLUMNAS Y LA ULTIMA. GUARDAR EN CSV SEPARADO POR "*" + SNAPPY

val lectura = sc.textFile("/user/cert/crimes/crimes.csv")
val cabecera = lectura.first
val cuerpo = lectura.filter(x => x != cabecera).map(x => x.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1))
val usecol = cuerpo.map(x => x(0) + "*" + x(1) + "*" + x(2)+ "*" + x(3) + "*" + x(4) + "*" + x(21))

usecol.saveAsTextFile("/user/cert/simulacro5/ej7", classOf[org.apache.hadoop.io.compress.GzipCodec])


// 8. CON PRODUCTS DEL EJERCICIO 1, MIRAR CUANTOS PRODUCTOS HAY. GUARDAR EN AVRO SIN COMPRESION.

import com.databricks.spark.avro._

val prod = sqlContext.read.avro("/user/cert/simulacro5/ej1")

prod.registerTempTable("distpro")

val sol = sqlContext.sql("select count(distinct(product_name)) as productos from distpro")

sqlContext.setConf("spark.sql.avro.compression.codec", "uncompressed")

sol.write.avro("/user/cert/simulacro5/ej8")

// 9. CONSEGUIR EL NUMERO DE ORDENES QUE SE HAN REALIZADO POR CLIENTE

import com.databricks.spark.avro._

val cust = sqlContext.read.avro("/user/cert/simulacro3/data/customers")

val oIt = sqlContext.read.avro("/user/cert/simulacro3/data/order_items")

val ord = sqlContext.read.avro("/user/cert/simulacro3/data/orders")

val joined = oIt.join(ord, oIt("order_item_order_id")===ord("order_id"))

val solucion = joined.groupBy("order_customer_id").agg(countDistinct("order_item_order_id").as("total_orders"))

solucion.write.parquet("/user/cert/simulacro5/ej9")