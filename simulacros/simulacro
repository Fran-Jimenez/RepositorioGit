//SIMULACRO

//1. IMPORTAR CUSTOMERS, EN FORMATO TEXTO, CON DELIMITADOR DE CAMPO TABULADOR, Y SALTO DE LINEA COMO DELIMITADOR DE LINEA. FILTRAR POR STATE = TX. 1 SOLO MAP

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table customers \
--target-dir /user/cert/simulacro/ej1 \
--as-textfile \
--fields-terminated-by '\t' \
--lines-terminated-by '\n' \
--where "customer_state = 'TX'" \
-m 1

//2. CREAR UNA TABLA EN MYSQL CON LOS NOMBRES Y TIPOS DEL FICHERO CRIMES DE 3 COLUMNAS(ID,Case Number,Date), Y EXPORTAR ESAS 3 COLUMNAS DESDE HDFS A LA TABLA

val crimes = sc.textFile("/user/cert/crimes/crimes.csv")
val cabecera = crimes.first
val cuerpo = crimes.filter(x => x != cabecera)
val threefields = cuerpo.map(_.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1)).map(x => x(0) + "," + x(1) + "," + x(2))
threefields.saveAsTextFile("/user/cert/simulacro/ej2")

mysql -u cloudera -pcloudera

create table ej2 (
ID varchar(255),
Case_Number varchar(255),
Date varchar(255))

sqoop export \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table ej2 \
--export-dir /user/cert/simulacro/ej2 \
--fields-terminated-by ','

//3. COGER LA TABLA resultado_csv DE LA BASE DE DATOS fbi DE HIVE, Y FILTRAR POR LOS AÑOS CON MAS DE 130000 DELITOS. GUARDAR COMO TEXTO CON DELIMITADOR TAB

hive

insert overwrite directory "/user/cert/simulacro/ej3" row format delimited fields terminated by '\t' stored as textfile select * from resultado_csv where delitos >= 130000;



//4. LEER EL JSON DE /user/cert/problem4/data FILTRAR POR itemType = Books. GUARDAR EN AVRO + SNAPPY

val jsondata = sqlContext.read.json("/user/cert/problem4/data")
val resultados = jsondata.where($"itemType"==='Books')

import com.databricks.spark.avro._
sqlContext.setConf("spark.sql.avro.compression.codec", "snappy")

resultados.write.avro("/user/cert/simulacro/ej4")

//5. DE LA TABALA CUSTOMER IMPORTADA EN EL 1, CONSEGUIR NOMBRE APELLIDO Y ORDENAR POR APELLIDO. GUARDAR EN FORMATO PARQUET

val custRDD = sc.textFile("/user/cert/simulacro/ej1")

val modify = custRDD.map(x => x.split('\t')).map(x => (x(2), x(1))).sortByKey(ascending = true).map(x => x._2 + " " + x._1)

import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

val estructura = StructType(Array(
	StructField("FL-Name", StringType)))

val campo = modify.map(x => Row(x))

myDF = sqlContext.createDataFrame(campo,estructura)

sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed")
myDF.write.parquet("/user/cert/simulacro/ej5")

//6. CON LAS TABLAS CUSTOMERS Y ORDERS, UNIR POR SU CAMPO COMUN, Y OBTENER DE SALIDA NOMBRE APELLIDO, Nº ORDENES

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table customers \
--target-dir /user/cert/simulacro/ej6/customers \
--as-parquetfile \
--fields-terminated-by ',' \
-m 1

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table orders \
--target-dir /user/cert/simulacro/ej6/orders \
--as-parquetfile \
--fields-terminated-by ',' \
-m 1

val custDF = sqlContext.read.parquet("/user/cert/simulacro/ej6/customers")
val orderDF = sqlContext.read.parquet("/user/cert/simulacro/ej6/orders")
val joinDF = custDF.join(orderDF, custDF("customer_id")===orderDF("order_customer_id"))

joinDF.rdd.map(x => (x(1),x(2), x(5), x(6)) ).map(x => (x._1 + " " + x._2, x._4 + " " + x._3)).map(x => x._1 + "," + x._2).saveAsTextFile("/user/cert/simulacro/ej6/resultado")

//7. DEL ARCHIVO CRIMES, QUEDARNOS CON LOS 7 PRIMEROS CAMPOS SEPARADOS POR '|'

val crimes = sc.textFile("/user/cert/crimes/crimes.csv")
val cabecera = crimes.first
val cuerpo = crimes.filter(x => x != cabecera)
val sevenfields = cuerpo.map(_.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1)).map(x => x(0) + "|" + x(1)+ "|" + x(2) + "|" + x(3) + "|" + x(4) + "|" + x(5) + "|" + x(6) )
sevenfields.saveAsTextFile("/user/cert/simulacro/ej7")

//8. EN LA TABLA CUSTOMERS, CONSEGUIR CUANTOS NOMBRES DISTINTOS HAY

val custRDD = sc.textFile("/user/cert/simulacro/ej1").map(_.split('\t')).map(x => x(1)).map(x => (x, 1)).reduceByKey((x,y) => x+y).count

//9. DE LA TABLA ORDER_ITEMS, SACAR CUANTOS PRODUCTOS SE HAN PEDIDO DE CADA TIPO, Y CUANTO SE HA PAGADO POR ELLOS


sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table order_items \
--target-dir /user/cert/simulacro/ej9/order_items \
--as-parquetfile \
--fields-terminated-by ',' \
--lines-terminated-by '\n' \
-m 1

val oItDF = sqlContext.read.parquet("/user/cert/simulacro/ej9/order_items")

val resultado = oItDF.groupBy("order_item_product_id").agg(countDistinct("order_item_order_id").as("total_order"), sum("order_item_quantity").as("total_products"), round(sum("order_item_subtotal"),2).as("total_price"))

import com.databricks.spark.avro._

sqlContext.setConf("spark.sql.avro.compression.codec", "gzip")