//SIMULACRO 2

// 1. IMPORTAR LA TABLA orders DE MYSQL EN FORMATO AVRO Y CON CONMPRESION SNAPPY, CON LOS CAMPOS DELIMITADOS POR ASTERISCOS. TRAER SOLO LOS REGISTROS DONDE order_status  SEA COMPLETE O CLOSED. 1 SOLO MAPPER.

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table orders \
--target-dir /user/cert/simulacro2/ej1 \
--fields-terminated-by '*' \
--where "order_status = 'COMPLETE' || order_status = 'CLOSED'" \
--as-avrodatafile \
--compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
-m 1

// 2. DEL ARCHIVO olympicAthletes.csv, REALIZAR UNA EXPORTACIÓN A MySQL(CREAR PREVIAMENTE LA TABLA) DE LAS COLUMNAS NOMBRE, EDAD, PAIS, TOTAL DE MEDALLAS

//ASUMIMOS QUE TENGO YA EL olympicAthletes.csv EN HDFS

val olympic = sc.textFile("/user/cert/simulacro2/ej2/data")

//NOS QUEDAMOS LOS CAMPOS QUE NOS INTERESAN

val usefields = olympic.map(_.split(',')).map(x => x(0) + "," + x(1) + "," + x(2) + "," + x(9) )

usefields.saveAsTextFile("/user/cert/simulacro2/ej2/exportacion")

//CREAMOS LA TABLA EN MYSQL
use retail_db;

create table olympic ( 
	name varchar(50), 
	age int, 
	country varchar(50), 
	medals int);

//HACEMOS EL EXPORT
sqoop export \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table olympic \
--export-dir /user/cert/simulacro2/ej2/exportacion \
--fields-terminated-by ','


// 3. CREAR UNA TABLA EN HIVE CON LOS ARCHIVOS DEL AVRO DEL EJERCICIO 1. CREAR LA TABLA USANDO EL ESQUEMA EMBEBIDO EN EL AVRO. REALIZAR UNA QUERY QUE DEVUELVA LOS REGISTROS CON ORDER_STATUS COMPLETE Y GUARDAR EL RESULTADO EN HDFS

hdfs dfs -get /user/cert/simulacro2/ej1/part-m-00000.avro 

avro-tools -getschema part-m-00000.avro > esquem.avsc

hdfs dfs -mkdir /user/cert/simulacro2/ej3/
hdfs dfs -mkdir /user/cert/simulacro2/ej3/esquema
hdfs dfs -put esquem.avsc /user/cert/simulacro2/ej3/esquema

//CREAMOS LA TABLA

hive

create external table orders_hive
stored as avro
location '/user/cert/simulacro2/ej1'
tblproperties ("avro.schema.url" = "/user/cert/simulacro2/ej3/esquema/esquem.avsc")
;


//HACEMOS LA QUERY Y LA GUARDAMOS EN HDFS

insert overwrite directory '/user/cert/simulacro2/ej3/solution' row format delimited fields terminated by ',' stored as textfile select * from orders_hive where order_status = 'COMPLETE';



// 4. USANDO ORDERS DEL EJERCICIO 1, QUEDARTE CON AQUELLO CON order_customer_id MENORES QUE 2000 Y GUARDAR EN FORMATO TSV CON GZIP

val ord = sqlContext.read.avro("/user/cert/simulacro2/ej1")

val filtro = ord.where($"order_customer_id" <= 2000)

val myRDD = filtro.rdd

val solution = myRDD.map(x => x(0) + "\t" + x(1) + "\t" + x(2) + "\t" + x(3))

solution.saveAsTextFile("/user/cert/simulacro2/ej4", classOf[org.apache.hadoop.io.compress.GzipCodec])

// 5. IMPORTAR CUSTOMERS DESDE MySQL EN FORMATO PARQUET+SNAPPY. JUNTO A LA TABLA ORDERS, CALCULAR EL TOP 5 DE CIUDADES CON MAYOR NUMERO DE ORDENES Y GUARDARLO EN CSV EN HDFS.

sqoop import \
--connect jdbc:mysql://localhost:3306/retail_db \
--username cloudera --password cloudera \
--table customers \
--target-dir /user/cert/simulacro2/ej5/customers \
--as-parquetfile \
--compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
-m 1

import com.databricks.spark.avro._

val ord = sqlContext.read.avro("/user/cert/simulacro2/ej1")

val cust = sqlContext.read.parquet("/user/cert/simulacro2/ej5/customers")

val joined = ord.join(cust, ord("order_customer_id") === cust("customer_id"))

val agrupado = joined.groupBy("customer_city").agg(count("order_customer_id").as("total_orders")).groupBy($"total_orders".desc)

agrupado.registerTempTable("tablero")

val solution = sqlContext.sql("select * from tablero limit 5")

solution.rdd.map(x => x(0) + "," + x(1)).saveAsTextFile("/user/cert/simulacro2/ej5/solution")

// 6. CONSEGUIR EL TOP DE PAISES CON MAS MEDALLAS DE olympicAthletes.csv USANDO RDDS. GUARDAR EN JSON SIN COMPRESION

val olympic = sc.textFile("/user/cert/simulacro2/ej2/data")

val pair = olympic.map(x=> x.split(',')).map(x => (x(2),x(9).toInt))

val solution = pair.reduceByKey((x,y) => x+y).map(x => (x._2,x._1)).sortByKey(ascending = false).map( x => (x._2,x._1))

//PARA GUARDAR EN JSON TENEMOS QUE DARLE UNA ESTRUCTURA
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

val estructura = StructType(Array(
  StructField("city", StringType),
  StructField("medals", IntegerType)
  ))

val campo = solution.map(x => Row(x._1, x._2.toInt))

val myDF = sqlContext.createDataFrame(campo, estructura)

myDF.repartition(1).toJSON.saveAsTextFile("/user/cert/simulacro2/ej6/solution")

// 7. COGER LAS ULTIMAS 5 COLUMNAS DEL FICHERO crimes.csv Y GUARDARLAS EN FICHERO DE TEXTO SEPARADO POR #, COMPRESION GZIP

val lectura = sc.textFile("/user/cert/crimes/crimes.csv")

val cabecera = lectura.first

val body = lectura.filter(x => x != cabecera)
val campos_utiles = body.map(x => x.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1)).map(x=>(x(17),x(18),x(19),x(20),x(21)))

val solucion = campos_utiles.map(x => x._1 + "#" + x._2 + "#" + x._3 + "#" + x._4 + "#" + x._5)

solucion.saveAsTextFile("/user/cert/simulacro2/ej7", classOf[org.apache.hadoop.io.compress.GzipCodec])


// 8. DEL FICHERO olympicAthletes.csv  ENCONTRAR CUANTAS PERSONAS POR PAIS SE HAN PRESENTADO. GUARDARLO EN PARQUET + SNAPPY

val olympic = sc.textFile("/user/cert/simulacro2/ej2/data").map(_.split(',')).map(x => (x(0),x(2)))

import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

val est = StructType(Array(
  StructField("name", StringType),
  StructField("country", StringType)
  ))

val campo = olympic.map(x => Row(x._1, x._2))

val myDf = sqlContext.createDataFrame(campo,est)

val solucion = myDf.groupBy("country").agg(countDistinct("name").as("medallistas")).orderBy($"medallistas".desc)

sqlContext.setConf("spark.sql.parquet.compression.codec", "snappy")

solucion.write.parquet("/user/cert/simulacro2/ej8")

// 9. CON LA TABLA ORDERS Y LA TABLA CUSTOMERS SACAR EL Nº DE ORDENES POR CODIGO POSTAL. GUARDAR EN AVRO

import com.databricks.spark.avro._

val ord = sqlContext.read.avro("/user/cert/simulacro2/ej1")

val cust = sqlContext.read.parquet("/user/cert/simulacro2/ej5/customers")

val joined = ord.join(cust, ord("order_customer_id")===cust("customer_id"))

val solucion = joined.groupBy("customer_zipcode").agg(count("order_customer_id").as("total_orders"))

solucion.write.avro("/user/cert/simulacro2/ej9")